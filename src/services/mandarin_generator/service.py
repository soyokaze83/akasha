"""Mandarin passage generation service."""

import logging
import re
from datetime import date
from typing import Optional

from src.core.config import settings
from src.llm import get_configured_llm

logger = logging.getLogger(__name__)


# Shared constants for message formatting
MESSAGE_HEADER_TEMPLATE = "ğŸ“š æ¯æ—¥ä¸­æ–‡é˜…è¯» - {date}"


def get_formatted_date() -> str:
    """Get today's date formatted for Mandarin passages (YYYYå¹´MMæœˆDDæ—¥)."""
    return date.today().strftime("%Yå¹´%mæœˆ%dæ—¥")


def format_passage_message(passage: str) -> str:
    """Format a passage with the standard header."""
    date_str = get_formatted_date()
    header = MESSAGE_HEADER_TEMPLATE.format(date=date_str)
    return f"{header}\n\n{passage}"

SYSTEM_INSTRUCTION = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡æ•™è‚²ä¸“å®¶, ä¸“é—¨ä¸ºä¸­çº§å­¦ä¹ è€… (HSK 3-4çº§) ç¼–å†™é˜…è¯»ææ–™ã€‚

è§„åˆ™: 
1. åªä½¿ç”¨HSK 3-4çº§çš„è¯æ±‡å’Œè¯­æ³•
2. åªè¾“å‡ºæ±‰å­—, ä¸è¦æ‹¼éŸ³, ä¸è¦è‹±æ–‡ç¿»è¯‘
3. æ–‡ç« é•¿åº¦: 300-500ä¸ªæ±‰å­— (è¿™æ˜¯ç¡¬æ€§è¦æ±‚, å¿…é¡»å†™å®Œæ•´)
4. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
5. å†…å®¹è¦æœ‰è¶£ã€å®ç”¨ã€è´´è¿‘ç”Ÿæ´»
6. æ–‡ç« ç»“æ„æ¸…æ™°, æœ‰å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾
7. å¯ä»¥é€‚å½“ä½¿ç”¨ä¸€äº›HSK 5çº§çš„ç®€å•è¯æ±‡, ä½†è¦ç¡®ä¿æ•´ä½“éš¾åº¦é€‚ä¸­
8. ä¸è¦åœ¨æ–‡ç« æœ«å°¾æ·»åŠ ä»»ä½•æ³¨é‡Šã€è¯æ±‡è¡¨æˆ–ç¿»è¯‘
9. ä¸è¦æ·»åŠ æ ‡é¢˜
10. å¿…é¡»å†™å®Œæ•´ç¯‡æ–‡ç« , ä¸è¦ä¸­é€”åœæ­¢"""

WEB_SEARCH_SYSTEM_INSTRUCTION = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡æ•™è‚²ä¸“å®¶, ä¸“é—¨ä¸ºä¸­çº§å­¦ä¹ è€… (HSK 3-4çº§) ç¼–å†™é˜…è¯»ææ–™ã€‚

ç‰¹åˆ«æ³¨æ„: ä½ å°†æ ¹æ®ä»Šå¤©çš„æ–°é—»/æ—¶äº‹çƒ­ç‚¹æ¥ç”Ÿæˆæ–‡ç« , æ‰€ä»¥è¯é¢˜å¯èƒ½ä¸æ—¥å¸¸ç”Ÿæ´»ä¸åŒ, ä½†éš¾åº¦å¿…é¡»ä¿æŒåœ¨HSK 3-4çº§ã€‚

è§„åˆ™: 
1. åªä½¿ç”¨HSK 3-4çº§çš„è¯æ±‡å’Œè¯­æ³•
2. åªè¾“å‡ºæ±‰å­—, ä¸è¦æ‹¼éŸ³, ä¸è¦è‹±æ–‡ç¿»è¯‘
3. æ–‡ç« é•¿åº¦: 300-500ä¸ªæ±‰å­— (è¿™æ˜¯ç¡¬æ€§è¦æ±‚, å¿…é¡»å†™å®Œæ•´) 
4. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
5. åŸºäºæä¾›çš„æ–°é—»/æ—¶äº‹çƒ­ç‚¹å†…å®¹é€‰æ‹©æœ‰è¶£è¯é¢˜
6. å†…å®¹è¦æœ‰è¶£ã€å®ç”¨ã€è´´è¿‘ç”Ÿæ´»
7. æ–‡ç« ç»“æ„æ¸…æ™°, æœ‰å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾
8. å¯ä»¥é€‚å½“ä½¿ç”¨ä¸€äº›HSK 5çº§çš„ç®€å•è¯æ±‡, ä½†è¦ç¡®ä¿æ•´ä½“éš¾åº¦é€‚ä¸­
9. ä¸è¦åœ¨æ–‡ç« æœ«å°¾æ·»åŠ ä»»ä½•æ³¨é‡Šã€è¯æ±‡è¡¨æˆ–ç¿»è¯‘
10. ä¸è¦æ·»åŠ æ ‡é¢˜
11. å¿…é¡»å†™å®Œæ•´ç¯‡æ–‡ç« , ä¸è¦ä¸­é€”åœæ­¢
12. æ ¹æ®æä¾›çš„æ–°é—»å†…å®¹è°ƒæ•´ç”Ÿæˆçš„æ–‡ç« , ä¸è¦æŠ„è¢­åŸæ–‡"""


class PassageGeneratorService:
    """Service for generating HSK 3-4 Mandarin reading passages."""

    # Passage length constraints (in Chinese characters)
    MIN_LENGTH = 250
    MAX_LENGTH = 600

    # Temperature for passage generation (0.9 = more creative, varied writing)
    # Higher temperature produces more diverse and interesting passages,
    # which is desirable for educational content that should feel fresh each day
    GENERATION_TEMPERATURE = 0.9

    def _validate_and_fix_length(self, passage: str) -> tuple[str, bool]:
        """
        Validate passage length and truncate if too long.

        Args:
            passage: The generated passage

        Returns:
            Tuple of (fixed_passage, is_valid) where is_valid indicates
            if the passage meets minimum length requirement
        """
        # Count only Chinese characters for length validation
        chinese_chars = len([c for c in passage if "\u4e00" <= c <= "\u9fff"])

        if chinese_chars < self.MIN_LENGTH:
            logger.warning(
                f"Passage too short: {chinese_chars} Chinese chars (min: {self.MIN_LENGTH})"
            )
            return passage, False

        if chinese_chars > self.MAX_LENGTH:
            logger.warning(
                f"Passage too long: {chinese_chars} Chinese chars (max: {self.MAX_LENGTH}), truncating"
            )
            # Truncate at sentence boundary (ã€‚ï¼ï¼Ÿ)
            passage = self._truncate_at_sentence(passage, self.MAX_LENGTH)

        return passage, True

    def _truncate_at_sentence(self, passage: str, max_chars: int) -> str:
        """Truncate passage at the nearest sentence boundary before max_chars."""
        # Find sentence endings (Chinese punctuation)
        sentence_endings = ["ã€‚", "ï¼", "ï¼Ÿ"]

        # Count Chinese characters and find truncation point
        char_count = 0
        last_sentence_end = 0

        for i, char in enumerate(passage):
            if "\u4e00" <= char <= "\u9fff":
                char_count += 1
            if char in sentence_endings:
                if char_count <= max_chars:
                    last_sentence_end = i + 1

            if char_count > max_chars and last_sentence_end > 0:
                break

        if last_sentence_end > 0:
            return passage[:last_sentence_end]

        # Fallback: hard truncate if no sentence boundary found
        return passage[:max_chars]

    async def _fetch_web_content(self) -> tuple[Optional[str], Optional[str]]:
        """
        Fetch web content for topic selection.

        Returns:
            Tuple of (page_content, failure_reason) - one will be None
        """
        from src.services.reply_agent.tools import web_search_tool
        from src.utils.web_scraper import fetch_page_text

        # Search for today's news in Chinese
        search_query = "ä»Šæ—¥æ–°é—» æœ‰è¶£çš„è¯é¢˜"
        logger.info(f"Searching for topics with query: {search_query}")

        results = await web_search_tool.search(search_query, num_results=3)

        if not results:
            return None, "No search results returned"

        # Fetch content from top result for LLM to analyze
        top_result = results[0]
        logger.info(f"Fetching content from: {top_result['title']}")

        page_content = await fetch_page_text(top_result["link"])

        # Limit to first 2000 characters to avoid overwhelming LLM
        if page_content and len(page_content) > 2000:
            page_content = page_content[:2000]
            logger.info("Limited page content to 2000 characters")

        if not page_content:
            logger.warning(
                f"Failed to fetch content from {top_result['link']}, using snippet instead"
            )
            page_content = top_result.get("snippet", "")

        if not page_content:
            return None, f"Failed to fetch content from {top_result['link']}"

        return page_content, None

    async def _generate_passage_from_web_content(
        self, page_content: str
    ) -> tuple[str, str]:
        """
        Generate passage and extract topic in a single LLM call.

        Args:
            page_content: Web content to base the passage on

        Returns:
            Tuple of (passage, topic)
        """
        # Combined prompt: select topic AND generate passage in one call
        combined_prompt = f"""åŸºäºä»¥ä¸‹æ–°é—»/ç½‘é¡µå†…å®¹, é€‰æ‹©ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜å¹¶å†™ä¸€ç¯‡çŸ­æ–‡ã€‚

æ–°é—»å†…å®¹:
{page_content}

è¦æ±‚:
1. åœ¨æ–‡ç« å¼€å¤´ç”¨ã€è¯é¢˜ï¼šXXXã€‘æ ‡æ³¨ä½ é€‰æ‹©çš„è¯é¢˜ (å¿…é¡»åŒ…å«è¿™ä¸ªæ ‡è®°)
2. è¯é¢˜è¦å…·ä½“ã€æœ‰è¶£ã€æ¥æºäºæä¾›çš„æ–°é—»å†…å®¹
3. é€‚åˆHSK 3-4çº§å­¦ä¹ è€…é˜…è¯»
4. åªç”¨æ±‰å­—, ä¸è¦æ‹¼éŸ³
5. 300-500ä¸ªæ±‰å­— (å¿…é¡»å†™å®Œæ•´)
6. å†…å®¹æœ‰è¶£ã€å®ç”¨
7. æ–‡ç« è¦æœ‰å®Œæ•´çš„å¼€å¤´ã€ä¸­é—´å’Œç»“å°¾

æ ¼å¼ç¤ºä¾‹:
ã€è¯é¢˜ï¼šæ˜¥èŠ‚æ—…æ¸¸ã€‘
æ˜¥èŠ‚å¿«åˆ°äº†, å¾ˆå¤šäººéƒ½åœ¨è®¡åˆ’å»æ—…æ¸¸...

ç›´æ¥è¾“å‡ºã€è¯é¢˜ï¼šXXXã€‘å’Œæ–‡ç« å†…å®¹ã€‚"""

        llm_client = get_configured_llm()
        response = await llm_client.generate_content(
            prompt=combined_prompt,
            system_instruction=WEB_SEARCH_SYSTEM_INSTRUCTION,
            temperature=self.GENERATION_TEMPERATURE,
        )

        response = response.strip()

        # Extract topic from ã€è¯é¢˜ï¼šXXXã€‘ pattern
        topic_match = re.search(r"ã€è¯é¢˜[ï¼š:]\s*(.+?)ã€‘", response)
        if topic_match:
            topic = topic_match.group(1).strip()
            # Remove topic marker from passage
            passage = re.sub(r"ã€è¯é¢˜[ï¼š:].*?ã€‘\s*", "", response).strip()
            logger.info(f"Extracted topic from combined response: '{topic}'")
        else:
            # Fallback if pattern not found
            topic = "ç½‘ç»œè¯é¢˜"
            passage = response
            logger.warning(
                "Could not extract topic from response, using default 'ç½‘ç»œè¯é¢˜'"
            )

        return passage, topic

    async def generate_passage(self, topic: Optional[str] = None) -> tuple[str, str]:
        """
        Generate a Mandarin reading passage.

        Args:
            topic: Optional specific topic for the passage

        Returns:
            Tuple of (passage text, topic used)
        """
        if topic:
            # Specific topic provided - use single LLM call
            prompt = f"""è¯·å†™ä¸€ç¯‡å…³äº"{topic}"çš„çŸ­æ–‡ã€‚

è¦æ±‚:
- é€‚åˆHSK 3-4çº§å­¦ä¹ è€…é˜…è¯»
- åªç”¨æ±‰å­—, ä¸è¦æ‹¼éŸ³
- 300-500ä¸ªæ±‰å­— (å¿…é¡»å†™å®Œæ•´)
- å†…å®¹æœ‰è¶£ã€å®ç”¨
- æ–‡ç« è¦æœ‰å®Œæ•´çš„å¼€å¤´ã€ä¸­é—´å’Œç»“å°¾

ç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹, ä¸è¦ä»»ä½•æ ‡é¢˜æˆ–é¢å¤–è¯´æ˜ã€‚"""

            llm_client = get_configured_llm()
            passage = await llm_client.generate_content(
                prompt=prompt,
                system_instruction=SYSTEM_INSTRUCTION,
                temperature=self.GENERATION_TEMPERATURE,
            )
            passage = passage.strip()
            display_topic = topic

        elif settings.topic_selection_mode == "web_search":
            # Web search mode: fetch content and generate in single LLM call
            page_content, failure_reason = await self._fetch_web_content()

            if page_content:
                # Single LLM call for both topic selection and passage generation
                passage, extracted_topic = await self._generate_passage_from_web_content(
                    page_content
                )
                display_topic = f"ç½‘ç»œè¯é¢˜: {extracted_topic}"
            else:
                # Fallback to free topic if web search fails
                logger.warning(f"Web search failed: {failure_reason}, falling back to free topic mode")
                passage, display_topic = await self._generate_free_topic_passage()
                display_topic = f"è‡ªç”±è¯é¢˜ (æœç´¢å¤±è´¥: {failure_reason})"
        else:
            # Free topic mode
            passage, display_topic = await self._generate_free_topic_passage()

        # Validate and fix passage length
        passage, is_valid = self._validate_and_fix_length(passage)

        # Count Chinese characters for logging
        chinese_chars = len([c for c in passage if "\u4e00" <= c <= "\u9fff"])
        logger.info(
            f"Generated passage for topic '{display_topic}': {chinese_chars} Chinese chars "
            f"(total: {len(passage)} chars, mode: {settings.topic_selection_mode}, "
            f"valid_length: {is_valid})"
        )
        return passage, display_topic

    async def _generate_free_topic_passage(self) -> tuple[str, str]:
        """Generate a passage with a freely chosen topic."""
        prompt = """è¯·è‡ªç”±é€‰æ‹©ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜, å†™ä¸€ç¯‡çŸ­æ–‡ã€‚è¯é¢˜å¯ä»¥æ˜¯ä»»ä½•å†…å®¹, æ¯”å¦‚: æ—¥å¸¸ç”Ÿæ´»ã€æ—…è¡Œç»å†ã€ç¾é£Ÿã€ç§‘æŠ€ã€æ–‡åŒ–ã€è‡ªç„¶ã€äººé™…å…³ç³»ã€å·¥ä½œå­¦ä¹ ã€å…´è¶£çˆ±å¥½ç­‰ç­‰ã€‚

è¦æ±‚:
- é€‚åˆHSK 3-4çº§å­¦ä¹ è€…é˜…è¯»
- åªç”¨æ±‰å­—, ä¸è¦æ‹¼éŸ³
- 300-500ä¸ªæ±‰å­— (å¿…é¡»å†™å®Œæ•´)
- å†…å®¹æœ‰è¶£ã€å®ç”¨
- æ–‡ç« è¦æœ‰å®Œæ•´çš„å¼€å¤´ã€ä¸­é—´å’Œç»“å°¾

ç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹, ä¸è¦ä»»ä½•æ ‡é¢˜æˆ–é¢å¤–è¯´æ˜ã€‚"""

        llm_client = get_configured_llm()
        passage = await llm_client.generate_content(
            prompt=prompt,
            system_instruction=SYSTEM_INSTRUCTION,
            temperature=self.GENERATION_TEMPERATURE,
        )
        return passage.strip(), "è‡ªç”±è¯é¢˜"


# Singleton instance
passage_generator = PassageGeneratorService()
